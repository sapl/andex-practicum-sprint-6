# Задание 1. Проектирование технологической архитектуры

- **[Ссылка на просмотр InureTech_технологическая архитектура_to-be.drawio](https://viewer.diagrams.net/?lightbox=1&highlight=0000ff&nav=1&title=InureTech_%D1%82%D0%B5%D1%85%D0%BD%D0%BE%D0%BB%D0%BE%D0%B3%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F%20%D0%B0%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80%D0%B0_to-be.drawio#Uhttps%3A%2F%2Fdrive.google.com%2Fuc%3Fid%3D1u29nOgBFtMPe7P7vAEKRBLWX9AIiKzkN%26export%3Ddownload)**  


### Гео распределение

Рассматривается российская компания с пользователями из РФ, поэтому датацентры будут только в России. Профиль сервиса
предполагает что реалтайм не требуется (биться за миллисекунды для дальневосточного региона особо нет задачи).
Поэтому оптимально использовать один кластер Kubernetes на два датацентра, например в Москве и Санкт-Петербурге. 
Это упростит развертывание и поддержку, обеспечивая надежность (нежели развертывать 2 независымых кластера)

#### CDN для статики

Для статики (JS, CSS, HTML, изображения и т.д.) целесообразно использовать CDN, задействуя все доступные регионы
провайдеров, от Спб до Новосибирска. Через CDN можно также кешировать общие API-ответы, что позволит снизить
нагрузку в пиковые моменты и ускорить загрузку продуктовых страниц.

### Фейловер-стратегия

Задача продублировать все узлы системы на случай как отказа отдельного сервера
так и на случай отключения электроэнергии в дата-центре. Для этого мы:

- Будем использовать один кластер кубернетис но в двух разных датацентрах.
- Предположительно в Санкт-Петербурге и Москве, слишком далеко разностить не стоит для снижения сетевых задержек.
- В кластере кубернетиса разворачиваем 5 мастер-нод, 3 в одном и 2 в другом датацентре и несколько воркер-нод в каждом.
- Запросы от пользователей будут идти на Nginx-сервера по одному в каждом датацентре
  Они балансируются через GeoDNS на ближайший датацентр.
  В случае отказа одного из Nginx запросы переключатся на второй 
- Nginx обращается Ingress балансер кубернетиса, который балансирует запросы между подами.
- Все поды имеют по крайней мере 3 реплики (в разных датацентрах и на разных физических серверах внутри одного).
- Коммуникация между репликами сервисов, по возможности будут внутри нод в том же датацентре (читал так можно настроить).
- У каждого пода есть health-чек который проверяет его работоспособность и в случае отказа убирает под из трафика,
- Также к кубернетис разворачиваем нфраструктурные сервисы в режиме кластера:
    - Кластер Kafka
    - Кластер Redis кеша
    - S3 хранилище
    - Сервисы мониторинга и логирования
- Для хранения данных продолжаем использовать PostgreSQL,
  но кластеризуем его используя Patroni, при этом Primary и Proxy должнен быть в каждом датацентре,
  чтобы в случае чего переключиться на другой датацентр.

### Масштабируемость и решение проблем с производительностью

- Используем авто-масштабирование кубернетис используя HPA вместе с Cluster Autoscaler
- Используем метрики RPS и числа заказов в Prometheus для настройки HPA для решения проблем с пиками
- Внетряем Event Driven архитектуру для уменьшения синхронных вызовов, для повышения стабильности
- Внедряем CQRS там где это применимо разделяя запросы на чтение и запись, часть данныз для чтения будет в Redis кеше
- Внедряем Rate limiting на корпоративное API ограничивая запросы оговоренными лимитами по клиентам и общим лимитом от ботов
- Внедряем GraphQL API для уменьшения числа запросов на сервер но только там где это обоснованно 
- Выделяем микросервисы из монолитов для более точечного тюнинга и масштабирования
- PostgreSQL кластеризуем используя Patroni для повышения отказоустойчивости и производительности
- Делаем ревизию метрик мониторинга, чтобы превентивно видеть проблемы 